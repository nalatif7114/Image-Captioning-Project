{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17780,
     "status": "ok",
     "timestamp": 1767146658311,
     "user": {
      "displayName": "Hafiz Anwar",
      "userId": "16627550854653570520"
     },
     "user_tz": -420
    },
    "id": "DUsya-quPcIb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116320,
     "status": "ok",
     "timestamp": 1767146774634,
     "user": {
      "displayName": "Hafiz Anwar",
      "userId": "16627550854653570520"
     },
     "user_tz": -420
    },
    "id": "1ouTP859PaMz",
    "outputId": "a50d7110-a78e-41e8-c037-65ebcea0d140"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# ============================================================================\n",
    "CSV_PATH = '../data/features/captions_preprocessed.csv'\n",
    "IMAGE_FEATURES_DIR = '../data/Images'\n",
    "TOKENIZER_PATH = '../data/features/tokenizer.pkl'\n",
    "CHECKPOINT_PATH = \"../checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7523,
     "status": "ok",
     "timestamp": 1767146782156,
     "user": {
      "displayName": "Hafiz Anwar",
      "userId": "16627550854653570520"
     },
     "user_tz": -420
    },
    "id": "_I9XvpxoPgmH"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD TOKENIZER & DATASET\n",
    "# ============================================================================\n",
    "# Memuat data tokenizer\n",
    "with open(TOKENIZER_PATH, 'rb') as f:\n",
    "    tokenizer_data = pickle.load(f)\n",
    "\n",
    "word_to_idx = tokenizer_data['word_to_idx']\n",
    "vocab_size = tokenizer_data['vocab_size']\n",
    "max_length = tokenizer_data['max_length']\n",
    "\n",
    "# Memuat dataset\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1767146782284,
     "user": {
      "displayName": "Hafiz Anwar",
      "userId": "16627550854653570520"
     },
     "user_tz": -420
    },
    "id": "Gn0UwhVnPtOg"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA GENERATOR\n",
    "# ============================================================================\n",
    "def load_data(img_name, caption):\n",
    "    # Menyesuaikan penamaan file .npy\n",
    "    path_npy = os.path.join(IMAGE_FEATURES_DIR, img_name.decode('utf-8') + '.npy')\n",
    "    img_tensor = np.load(path_npy)\n",
    "\n",
    "    # Tokenisasi caption\n",
    "    cap_seq = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in caption.decode('utf-8').split()]\n",
    "    cap_seq = tf.keras.preprocessing.sequence.pad_sequences([cap_seq], maxlen=max_length, padding='post')[0]\n",
    "\n",
    "    return img_tensor, cap_seq\n",
    "\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor, cap_seq = tf.numpy_function(load_data, [img_name, cap], [tf.float32, tf.int32])\n",
    "    img_tensor.set_shape((64, 2048)) # Sesuai output InceptionV3 yang di-reshape\n",
    "    cap_seq.set_shape((max_length,))\n",
    "    return img_tensor, cap_seq\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_df['image'].values, train_df['caption'].values))\n",
    "dataset = dataset.map(map_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1767146782336,
     "user": {
      "displayName": "Hafiz Anwar",
      "userId": "16627550854653570520"
     },
     "user_tz": -420
    },
    "id": "tt0NICTIP10o"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "        # Menangani range fitur (0-17) agar stabil\n",
    "        self.ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        # Mencegah Dead ReLU pada fitur yang banyak nol\n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.ln(x)\n",
    "        return self.leaky_relu(x)\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True)\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, _ = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        x = self.fc1(output)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        return self.fc2(x), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1559,
     "status": "ok",
     "timestamp": 1767146783897,
     "user": {
      "displayName": "Hafiz Anwar",
      "userId": "16627550854653570520"
     },
     "user_tz": -420
    },
    "id": "qbebzZ0EQDvP",
    "outputId": "f98cd322-e87d-42cc-9580-89bd1c971a61"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING SETUP\n",
    "# ============================================================================\n",
    "encoder = CNN_Encoder(EMBEDDING_DIM)\n",
    "decoder = RNN_Decoder(EMBEDDING_DIM, UNITS, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # Masking padding index 0\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / (tf.reduce_sum(mask) + 1e-8)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_PATH, max_to_keep=5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = tf.zeros((target.shape[0], UNITS))\n",
    "    dec_input = tf.expand_dims([word_to_idx['startseq']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # Hitung gradien\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    # Gradient Clipping untuk mencegah NaN\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d7510a8",
    "outputId": "8dd3939d-e322-4e64-a1b3-b5260be4a440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1 Batch 0 Loss 2.2978\n",
      "Epoch 1 Batch 50 Loss 1.5712\n",
      "Epoch 1 Batch 100 Loss 2.6343\n",
      "Epoch 1 Batch 150 Loss 1.5109\n",
      "Epoch 1 Batch 200 Loss 1.6656\n",
      "Epoch 1 Batch 250 Loss 1.8083\n",
      "Epoch 1 Batch 300 Loss 3.4815\n",
      "Epoch 1 Batch 350 Loss 1.8427\n",
      "Epoch 1 Batch 400 Loss 1.4981\n",
      "Epoch 1 Batch 450 Loss 1.8567\n",
      "Epoch 1 Batch 500 Loss 1.9831\n",
      "Epoch 1 Batch 550 Loss 2.1971\n",
      "Epoch 1 Final Loss 2.1044\n",
      "Time taken: 301.10 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.7112\n",
      "Epoch 2 Batch 50 Loss 2.2610\n",
      "Epoch 2 Batch 100 Loss 2.5404\n",
      "Epoch 2 Batch 150 Loss 1.8573\n",
      "Epoch 2 Batch 200 Loss 1.8133\n",
      "Epoch 2 Batch 250 Loss 1.6917\n",
      "Epoch 2 Batch 300 Loss 1.5860\n",
      "Epoch 2 Batch 350 Loss 2.6071\n",
      "Epoch 2 Batch 400 Loss 2.5245\n",
      "Epoch 2 Batch 450 Loss 3.0042\n",
      "Epoch 2 Batch 500 Loss 1.4400\n",
      "Epoch 2 Batch 550 Loss 1.6534\n",
      "Epoch 2 Final Loss 1.9949\n",
      "Time taken: 347.25 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.4914\n",
      "Epoch 3 Batch 50 Loss 1.7848\n",
      "Epoch 3 Batch 100 Loss 1.7141\n",
      "Epoch 3 Batch 150 Loss 1.7679\n",
      "Epoch 3 Batch 200 Loss 1.8926\n",
      "Epoch 3 Batch 250 Loss 2.6751\n",
      "Epoch 3 Batch 300 Loss 1.8647\n",
      "Epoch 3 Batch 350 Loss 2.5525\n",
      "Epoch 3 Batch 400 Loss 1.7328\n",
      "Epoch 3 Batch 450 Loss 1.8298\n",
      "Epoch 3 Batch 500 Loss 2.1027\n",
      "Epoch 3 Batch 550 Loss 2.6233\n",
      "Epoch 3 Final Loss 1.9367\n",
      "Time taken: 363.67 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.7613\n",
      "Epoch 4 Batch 50 Loss 1.3537\n",
      "Epoch 4 Batch 100 Loss 1.4728\n",
      "Epoch 4 Batch 150 Loss 2.0263\n",
      "Epoch 4 Batch 200 Loss 1.6016\n",
      "Epoch 4 Batch 250 Loss 2.0666\n",
      "Epoch 4 Batch 300 Loss 1.7748\n",
      "Epoch 4 Batch 350 Loss 2.1676\n",
      "Epoch 4 Batch 400 Loss 1.6722\n",
      "Epoch 4 Batch 450 Loss 1.3735\n",
      "Epoch 4 Batch 500 Loss 2.0754\n",
      "Epoch 4 Batch 550 Loss 1.8079\n",
      "Epoch 4 Final Loss 1.8691\n",
      "Time taken: 306.09 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.6098\n",
      "Epoch 5 Batch 50 Loss 1.4793\n",
      "Epoch 5 Batch 100 Loss 1.8784\n",
      "Epoch 5 Batch 150 Loss 2.1670\n",
      "Epoch 5 Batch 200 Loss 2.1279\n",
      "Epoch 5 Batch 250 Loss 1.9158\n",
      "Epoch 5 Batch 300 Loss 1.3474\n",
      "Epoch 5 Batch 350 Loss 1.4854\n",
      "Epoch 5 Batch 400 Loss 1.7359\n",
      "Epoch 5 Batch 450 Loss 1.7242\n",
      "Epoch 5 Batch 500 Loss 1.7446\n",
      "Epoch 5 Batch 550 Loss 1.5411\n",
      "Epoch 5 Final Loss 1.8129\n",
      "Time taken: 320.57 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.7081\n",
      "Epoch 6 Batch 50 Loss 1.2905\n",
      "Epoch 6 Batch 100 Loss 2.1039\n",
      "Epoch 6 Batch 150 Loss 1.6492\n",
      "Epoch 6 Batch 200 Loss 1.6339\n",
      "Epoch 6 Batch 250 Loss 1.3098\n",
      "Epoch 6 Batch 300 Loss 1.9543\n",
      "Epoch 6 Batch 350 Loss 2.5352\n",
      "Epoch 6 Batch 400 Loss 1.8507\n",
      "Epoch 6 Batch 450 Loss 1.9975\n",
      "Epoch 6 Batch 500 Loss 1.6211\n",
      "Epoch 6 Batch 550 Loss 1.3993\n",
      "Epoch 6 Final Loss 1.7556\n",
      "Time taken: 279.68 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.6381\n",
      "Epoch 7 Batch 50 Loss 1.5417\n",
      "Epoch 7 Batch 100 Loss 1.8529\n",
      "Epoch 7 Batch 150 Loss 1.7468\n",
      "Epoch 7 Batch 200 Loss 1.3255\n",
      "Epoch 7 Batch 250 Loss 1.7843\n",
      "Epoch 7 Batch 300 Loss 1.5860\n",
      "Epoch 7 Batch 350 Loss 1.7624\n",
      "Epoch 7 Batch 400 Loss 1.4369\n",
      "Epoch 7 Batch 450 Loss 1.6293\n",
      "Epoch 7 Batch 500 Loss 1.7589\n",
      "Epoch 7 Batch 550 Loss 1.9923\n",
      "Epoch 7 Final Loss 1.7118\n",
      "Time taken: 331.43 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.4900\n",
      "Epoch 8 Batch 50 Loss 2.0219\n",
      "Epoch 8 Batch 100 Loss 1.8360\n",
      "Epoch 8 Batch 150 Loss 1.2207\n",
      "Epoch 8 Batch 200 Loss 1.5680\n",
      "Epoch 8 Batch 250 Loss 1.4862\n",
      "Epoch 8 Batch 300 Loss 1.4881\n",
      "Epoch 8 Batch 350 Loss 1.6662\n",
      "Epoch 8 Batch 400 Loss 1.7104\n",
      "Epoch 8 Batch 450 Loss 1.5945\n",
      "Epoch 8 Batch 500 Loss 1.7970\n",
      "Epoch 8 Batch 550 Loss 2.0366\n",
      "Epoch 8 Final Loss 1.6699\n",
      "Time taken: 352.93 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.4550\n",
      "Epoch 9 Batch 50 Loss 1.3696\n",
      "Epoch 9 Batch 100 Loss 1.4753\n",
      "Epoch 9 Batch 150 Loss 1.9552\n",
      "Epoch 9 Batch 200 Loss 1.5458\n",
      "Epoch 9 Batch 250 Loss 1.6201\n",
      "Epoch 9 Batch 300 Loss 1.4678\n",
      "Epoch 9 Batch 350 Loss 1.9972\n",
      "Epoch 9 Batch 400 Loss 1.7747\n",
      "Epoch 9 Batch 450 Loss 1.7456\n",
      "Epoch 9 Batch 500 Loss 1.6311\n",
      "Epoch 9 Batch 550 Loss 1.5676\n",
      "Epoch 9 Final Loss 1.6222\n",
      "Time taken: 303.00 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.5339\n",
      "Epoch 10 Batch 50 Loss 1.5238\n",
      "Epoch 10 Batch 100 Loss 1.5327\n",
      "Epoch 10 Batch 150 Loss 1.7682\n",
      "Epoch 10 Batch 200 Loss 1.7058\n",
      "Epoch 10 Batch 250 Loss 1.2571\n",
      "Epoch 10 Batch 300 Loss 2.3470\n",
      "Epoch 10 Batch 350 Loss 1.6210\n",
      "Epoch 10 Batch 400 Loss 1.5038\n",
      "Epoch 10 Batch 450 Loss 1.4546\n",
      "Epoch 10 Batch 500 Loss 1.6903\n",
      "Epoch 10 Batch 550 Loss 2.1906\n",
      "Epoch 10 Final Loss 1.5876\n",
      "Time taken: 345.75 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.7174\n",
      "Epoch 11 Batch 50 Loss 1.6223\n",
      "Epoch 11 Batch 100 Loss 1.6854\n",
      "Epoch 11 Batch 150 Loss 1.4548\n",
      "Epoch 11 Batch 200 Loss 1.2690\n",
      "Epoch 11 Batch 250 Loss 1.5344\n",
      "Epoch 11 Batch 300 Loss 1.4406\n",
      "Epoch 11 Batch 350 Loss 1.4989\n",
      "Epoch 11 Batch 400 Loss 1.6956\n",
      "Epoch 11 Batch 450 Loss 1.7152\n",
      "Epoch 11 Batch 500 Loss 2.0437\n",
      "Epoch 11 Batch 550 Loss 1.4955\n",
      "Epoch 11 Final Loss 1.5390\n",
      "Time taken: 331.06 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.6111\n",
      "Epoch 12 Batch 50 Loss 1.4250\n",
      "Epoch 12 Batch 100 Loss 1.3431\n",
      "Epoch 12 Batch 150 Loss 1.4183\n",
      "Epoch 12 Batch 200 Loss 1.3889\n",
      "Epoch 12 Batch 250 Loss 1.4082\n",
      "Epoch 12 Batch 300 Loss 1.7404\n",
      "Epoch 12 Batch 350 Loss 1.6626\n",
      "Epoch 12 Batch 400 Loss 1.4278\n",
      "Epoch 12 Batch 450 Loss 1.4240\n",
      "Epoch 12 Batch 500 Loss 1.3004\n",
      "Epoch 12 Batch 550 Loss 1.3888\n",
      "Epoch 12 Final Loss 1.5073\n",
      "Time taken: 278.65 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.2392\n",
      "Epoch 13 Batch 50 Loss 1.4899\n",
      "Epoch 13 Batch 100 Loss 1.4192\n",
      "Epoch 13 Batch 150 Loss 1.5847\n",
      "Epoch 13 Batch 200 Loss 1.3268\n",
      "Epoch 13 Batch 250 Loss 1.7013\n",
      "Epoch 13 Batch 300 Loss 1.6179\n",
      "Epoch 13 Batch 350 Loss 1.7148\n",
      "Epoch 13 Batch 400 Loss 1.5543\n",
      "Epoch 13 Batch 450 Loss 1.7447\n",
      "Epoch 13 Batch 500 Loss 1.8296\n",
      "Epoch 13 Batch 550 Loss 1.3115\n",
      "Epoch 13 Final Loss 1.4698\n",
      "Time taken: 273.91 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.3541\n",
      "Epoch 14 Batch 50 Loss 1.2115\n",
      "Epoch 14 Batch 100 Loss 1.4510\n",
      "Epoch 14 Batch 150 Loss 1.7219\n",
      "Epoch 14 Batch 200 Loss 1.2324\n",
      "Epoch 14 Batch 250 Loss 1.2945\n",
      "Epoch 14 Batch 300 Loss 1.2655\n",
      "Epoch 14 Batch 350 Loss 1.5092\n",
      "Epoch 14 Batch 400 Loss 1.8530\n",
      "Epoch 14 Batch 450 Loss 1.5710\n",
      "Epoch 14 Batch 500 Loss 1.2585\n",
      "Epoch 14 Batch 550 Loss 1.1159\n",
      "Epoch 14 Final Loss 1.4364\n",
      "Time taken: 268.04 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.4225\n",
      "Epoch 15 Batch 50 Loss 1.3510\n",
      "Epoch 15 Batch 100 Loss 1.6701\n",
      "Epoch 15 Batch 150 Loss 1.4827\n",
      "Epoch 15 Batch 200 Loss 1.3528\n",
      "Epoch 15 Batch 250 Loss 1.3444\n",
      "Epoch 15 Batch 300 Loss 1.4055\n",
      "Epoch 15 Batch 350 Loss 1.2252\n",
      "Epoch 15 Batch 400 Loss 1.4245\n",
      "Epoch 15 Batch 450 Loss 1.3609\n",
      "Epoch 15 Batch 500 Loss 1.2117\n",
      "Epoch 15 Batch 550 Loss 1.3976\n",
      "Epoch 15 Final Loss 1.4026\n",
      "Time taken: 271.62 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.3236\n",
      "Epoch 16 Batch 50 Loss 1.2678\n",
      "Epoch 16 Batch 100 Loss 1.1420\n",
      "Epoch 16 Batch 150 Loss 1.1315\n",
      "Epoch 16 Batch 200 Loss 1.6318\n",
      "Epoch 16 Batch 250 Loss 1.2089\n",
      "Epoch 16 Batch 300 Loss 1.1695\n",
      "Epoch 16 Batch 350 Loss 1.3297\n",
      "Epoch 16 Batch 400 Loss 1.6924\n",
      "Epoch 16 Batch 450 Loss 1.5114\n",
      "Epoch 16 Batch 500 Loss 1.1709\n",
      "Epoch 16 Batch 550 Loss 1.2899\n",
      "Epoch 16 Final Loss 1.3814\n",
      "Time taken: 274.28 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.1933\n",
      "Epoch 17 Batch 50 Loss 1.3042\n",
      "Epoch 17 Batch 100 Loss 1.2364\n",
      "Epoch 17 Batch 150 Loss 1.2385\n",
      "Epoch 17 Batch 200 Loss 1.3114\n",
      "Epoch 17 Batch 250 Loss 1.3080\n",
      "Epoch 17 Batch 300 Loss 1.1279\n",
      "Epoch 17 Batch 350 Loss 1.3349\n",
      "Epoch 17 Batch 400 Loss 1.3070\n",
      "Epoch 17 Batch 450 Loss 1.9917\n",
      "Epoch 17 Batch 500 Loss 1.5524\n",
      "Epoch 17 Batch 550 Loss 1.6691\n",
      "Epoch 17 Final Loss 1.3456\n",
      "Time taken: 262.53 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.1407\n",
      "Epoch 18 Batch 50 Loss 1.5078\n",
      "Epoch 18 Batch 100 Loss 1.3607\n",
      "Epoch 18 Batch 150 Loss 1.0712\n",
      "Epoch 18 Batch 200 Loss 1.0574\n",
      "Epoch 18 Batch 250 Loss 1.3415\n",
      "Epoch 18 Batch 300 Loss 1.1290\n",
      "Epoch 18 Batch 350 Loss 1.3589\n",
      "Epoch 18 Batch 400 Loss 1.1565\n",
      "Epoch 18 Batch 450 Loss 1.2226\n",
      "Epoch 18 Batch 500 Loss 1.3093\n",
      "Epoch 18 Batch 550 Loss 1.3493\n",
      "Epoch 18 Final Loss 1.3170\n",
      "Time taken: 263.04 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.4588\n",
      "Epoch 19 Batch 50 Loss 1.2604\n",
      "Epoch 19 Batch 100 Loss 1.5543\n",
      "Epoch 19 Batch 150 Loss 1.1545\n",
      "Epoch 19 Batch 200 Loss 1.3299\n",
      "Epoch 19 Batch 250 Loss 1.3746\n",
      "Epoch 19 Batch 300 Loss 1.1740\n",
      "Epoch 19 Batch 350 Loss 1.3716\n",
      "Epoch 19 Batch 400 Loss 1.5299\n",
      "Epoch 19 Batch 450 Loss 1.2151\n",
      "Epoch 19 Batch 500 Loss 1.2851\n",
      "Epoch 19 Batch 550 Loss 1.4884\n",
      "Epoch 19 Final Loss 1.2953\n",
      "Time taken: 261.46 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.4984\n",
      "Epoch 20 Batch 50 Loss 1.2053\n",
      "Epoch 20 Batch 100 Loss 1.4831\n",
      "Epoch 20 Batch 150 Loss 1.0774\n",
      "Epoch 20 Batch 200 Loss 1.0113\n",
      "Epoch 20 Batch 250 Loss 1.0908\n",
      "Epoch 20 Batch 300 Loss 1.1936\n",
      "Epoch 20 Batch 350 Loss 1.3761\n",
      "Epoch 20 Batch 400 Loss 1.3033\n",
      "Epoch 20 Batch 450 Loss 1.3934\n",
      "Epoch 20 Batch 500 Loss 1.3467\n",
      "Epoch 20 Batch 550 Loss 1.2633\n",
      "Epoch 20 Final Loss 1.2715\n",
      "Time taken: 265.64 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.2385\n",
      "Epoch 21 Batch 50 Loss 1.3346\n",
      "Epoch 21 Batch 100 Loss 1.1555\n",
      "Epoch 21 Batch 150 Loss 1.2393\n",
      "Epoch 21 Batch 200 Loss 1.1390\n",
      "Epoch 21 Batch 250 Loss 1.3507\n",
      "Epoch 21 Batch 300 Loss 1.1586\n",
      "Epoch 21 Batch 350 Loss 1.4503\n",
      "Epoch 21 Batch 400 Loss 1.2183\n",
      "Epoch 21 Batch 450 Loss 1.2190\n",
      "Epoch 21 Batch 500 Loss 1.1519\n",
      "Epoch 21 Batch 550 Loss 1.0773\n",
      "Epoch 21 Final Loss 1.2520\n",
      "Time taken: 316.00 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.1769\n",
      "Epoch 22 Batch 50 Loss 1.3594\n",
      "Epoch 22 Batch 100 Loss 1.1732\n",
      "Epoch 22 Batch 150 Loss 1.1188\n",
      "Epoch 22 Batch 200 Loss 1.1140\n",
      "Epoch 22 Batch 250 Loss 1.5970\n",
      "Epoch 22 Batch 300 Loss 1.0828\n",
      "Epoch 22 Batch 350 Loss 1.2105\n",
      "Epoch 22 Batch 400 Loss 1.2715\n",
      "Epoch 22 Batch 450 Loss 1.1977\n",
      "Epoch 22 Batch 500 Loss 1.3189\n",
      "Epoch 22 Batch 550 Loss 1.2989\n",
      "Epoch 22 Final Loss 1.2266\n",
      "Time taken: 301.62 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.1193\n",
      "Epoch 23 Batch 50 Loss 1.3294\n",
      "Epoch 23 Batch 100 Loss 1.1630\n",
      "Epoch 23 Batch 150 Loss 1.3659\n",
      "Epoch 23 Batch 200 Loss 1.1854\n",
      "Epoch 23 Batch 250 Loss 1.2762\n",
      "Epoch 23 Batch 300 Loss 1.1571\n",
      "Epoch 23 Batch 350 Loss 1.1425\n",
      "Epoch 23 Batch 400 Loss 1.3191\n",
      "Epoch 23 Batch 450 Loss 1.2666\n",
      "Epoch 23 Batch 500 Loss 1.1648\n",
      "Epoch 23 Batch 550 Loss 1.1774\n",
      "Epoch 23 Final Loss 1.2038\n",
      "Time taken: 292.64 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.1237\n",
      "Epoch 24 Batch 50 Loss 1.1466\n",
      "Epoch 24 Batch 100 Loss 1.1334\n",
      "Epoch 24 Batch 150 Loss 1.1138\n",
      "Epoch 24 Batch 200 Loss 0.9926\n",
      "Epoch 24 Batch 250 Loss 1.1805\n",
      "Epoch 24 Batch 300 Loss 1.3219\n",
      "Epoch 24 Batch 350 Loss 1.2302\n",
      "Epoch 24 Batch 400 Loss 1.2457\n",
      "Epoch 24 Batch 450 Loss 1.1934\n",
      "Epoch 24 Batch 500 Loss 0.9966\n",
      "Epoch 24 Batch 550 Loss 1.1239\n",
      "Epoch 24 Final Loss 1.1806\n",
      "Time taken: 288.81 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.2018\n",
      "Epoch 25 Batch 50 Loss 1.0112\n",
      "Epoch 25 Batch 100 Loss 1.1132\n",
      "Epoch 25 Batch 150 Loss 1.1677\n",
      "Epoch 25 Batch 200 Loss 1.1203\n",
      "Epoch 25 Batch 250 Loss 1.1615\n",
      "Epoch 25 Batch 300 Loss 1.4238\n",
      "Epoch 25 Batch 350 Loss 1.1546\n",
      "Epoch 25 Batch 400 Loss 1.3425\n",
      "Epoch 25 Batch 450 Loss 1.0817\n",
      "Epoch 25 Batch 500 Loss 1.2573\n",
      "Epoch 25 Batch 550 Loss 1.1989\n",
      "Epoch 25 Final Loss 1.1611\n",
      "Time taken: 283.04 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.1914\n",
      "Epoch 26 Batch 50 Loss 1.2744\n",
      "Epoch 26 Batch 100 Loss 1.0952\n",
      "Epoch 26 Batch 150 Loss 1.2778\n",
      "Epoch 26 Batch 200 Loss 1.0170\n",
      "Epoch 26 Batch 250 Loss 1.1172\n",
      "Epoch 26 Batch 300 Loss 1.1347\n",
      "Epoch 26 Batch 350 Loss 1.2975\n",
      "Epoch 26 Batch 400 Loss 1.2006\n",
      "Epoch 26 Batch 450 Loss 1.3126\n",
      "Epoch 26 Batch 500 Loss 1.1913\n",
      "Epoch 26 Batch 550 Loss 1.1626\n",
      "Epoch 26 Final Loss 1.1430\n",
      "Time taken: 284.31 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.1367\n",
      "Epoch 27 Batch 50 Loss 1.0315\n",
      "Epoch 27 Batch 100 Loss 1.0720\n",
      "Epoch 27 Batch 150 Loss 1.1880\n",
      "Epoch 27 Batch 200 Loss 1.0985\n",
      "Epoch 27 Batch 250 Loss 1.0838\n",
      "Epoch 27 Batch 300 Loss 1.0838\n",
      "Epoch 27 Batch 350 Loss 1.0902\n",
      "Epoch 27 Batch 400 Loss 1.0902\n",
      "Epoch 27 Batch 450 Loss 1.1188\n",
      "Epoch 27 Batch 500 Loss 1.3398\n",
      "Epoch 27 Batch 550 Loss 1.1444\n",
      "Epoch 27 Final Loss 1.1239\n",
      "Time taken: 310.03 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 1.1311\n",
      "Epoch 28 Batch 50 Loss 1.1007\n",
      "Epoch 28 Batch 100 Loss 1.1690\n",
      "Epoch 28 Batch 150 Loss 1.0089\n",
      "Epoch 28 Batch 200 Loss 1.0712\n",
      "Epoch 28 Batch 250 Loss 1.0627\n",
      "Epoch 28 Batch 300 Loss 1.0747\n",
      "Epoch 28 Batch 350 Loss 1.0510\n",
      "Epoch 28 Batch 400 Loss 1.0277\n",
      "Epoch 28 Batch 450 Loss 1.1264\n",
      "Epoch 28 Batch 500 Loss 1.0342\n",
      "Epoch 28 Batch 550 Loss 0.9596\n",
      "Epoch 28 Final Loss 1.1058\n",
      "Time taken: 293.24 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 1.0074\n",
      "Epoch 29 Batch 50 Loss 1.0939\n",
      "Epoch 29 Batch 100 Loss 1.2507\n",
      "Epoch 29 Batch 150 Loss 1.0989\n",
      "Epoch 29 Batch 200 Loss 0.9666\n",
      "Epoch 29 Batch 250 Loss 1.1435\n",
      "Epoch 29 Batch 300 Loss 1.1632\n",
      "Epoch 29 Batch 350 Loss 1.1364\n",
      "Epoch 29 Batch 400 Loss 1.1039\n",
      "Epoch 29 Batch 450 Loss 1.0900\n",
      "Epoch 29 Batch 500 Loss 1.1043\n",
      "Epoch 29 Batch 550 Loss 1.1290\n",
      "Epoch 29 Final Loss 1.0915\n",
      "Time taken: 293.22 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 1.0917\n",
      "Epoch 30 Batch 50 Loss 1.0928\n",
      "Epoch 30 Batch 100 Loss 1.0684\n",
      "Epoch 30 Batch 150 Loss 1.1890\n",
      "Epoch 30 Batch 200 Loss 0.9975\n",
      "Epoch 30 Batch 250 Loss 1.2808\n",
      "Epoch 30 Batch 300 Loss 1.1796\n",
      "Epoch 30 Batch 350 Loss 1.0010\n",
      "Epoch 30 Batch 400 Loss 1.1470\n",
      "Epoch 30 Batch 450 Loss 1.0967\n",
      "Epoch 30 Batch 500 Loss 1.0050\n",
      "Epoch 30 Batch 550 Loss 1.0429\n",
      "Epoch 30 Final Loss 1.0715\n",
      "Time taken: 296.78 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 1.0290\n",
      "Epoch 31 Batch 50 Loss 1.0010\n",
      "Epoch 31 Batch 100 Loss 0.8967\n",
      "Epoch 31 Batch 150 Loss 1.0630\n",
      "Epoch 31 Batch 200 Loss 0.9558\n",
      "Epoch 31 Batch 250 Loss 0.9732\n",
      "Epoch 31 Batch 300 Loss 1.0929\n",
      "Epoch 31 Batch 350 Loss 1.0107\n",
      "Epoch 31 Batch 400 Loss 1.1460\n",
      "Epoch 31 Batch 450 Loss 0.9319\n",
      "Epoch 31 Batch 500 Loss 1.0305\n",
      "Epoch 31 Batch 550 Loss 1.1340\n",
      "Epoch 31 Final Loss 1.0558\n",
      "Time taken: 287.29 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 1.0260\n",
      "Epoch 32 Batch 50 Loss 0.9670\n",
      "Epoch 32 Batch 100 Loss 1.1401\n",
      "Epoch 32 Batch 150 Loss 0.9690\n",
      "Epoch 32 Batch 200 Loss 0.9852\n",
      "Epoch 32 Batch 250 Loss 1.0072\n",
      "Epoch 32 Batch 300 Loss 1.1293\n",
      "Epoch 32 Batch 350 Loss 1.1035\n",
      "Epoch 32 Batch 400 Loss 1.1926\n",
      "Epoch 32 Batch 450 Loss 1.0510\n",
      "Epoch 32 Batch 500 Loss 1.1188\n",
      "Epoch 32 Batch 550 Loss 0.9412\n",
      "Epoch 32 Final Loss 1.0396\n",
      "Time taken: 283.37 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.9934\n",
      "Epoch 33 Batch 50 Loss 0.9540\n",
      "Epoch 33 Batch 100 Loss 0.9327\n",
      "Epoch 33 Batch 150 Loss 0.9843\n",
      "Epoch 33 Batch 200 Loss 1.0550\n",
      "Epoch 33 Batch 250 Loss 1.3865\n",
      "Epoch 33 Batch 300 Loss 1.0286\n",
      "Epoch 33 Batch 350 Loss 1.1484\n",
      "Epoch 33 Batch 400 Loss 1.1440\n",
      "Epoch 33 Batch 450 Loss 1.0741\n",
      "Epoch 33 Batch 500 Loss 0.8819\n",
      "Epoch 33 Batch 550 Loss 1.0669\n",
      "Epoch 33 Final Loss 1.0236\n",
      "Time taken: 252.28 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 1.0913\n",
      "Epoch 34 Batch 50 Loss 1.0222\n",
      "Epoch 34 Batch 100 Loss 1.0259\n",
      "Epoch 34 Batch 150 Loss 0.9888\n",
      "Epoch 34 Batch 200 Loss 0.9378\n",
      "Epoch 34 Batch 250 Loss 1.1852\n",
      "Epoch 34 Batch 300 Loss 1.0774\n",
      "Epoch 34 Batch 350 Loss 0.8710\n",
      "Epoch 34 Batch 400 Loss 0.9551\n",
      "Epoch 34 Batch 450 Loss 0.9595\n",
      "Epoch 34 Batch 500 Loss 0.9799\n",
      "Epoch 34 Batch 550 Loss 0.9159\n",
      "Epoch 34 Final Loss 1.0079\n",
      "Time taken: 258.54 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.9087\n",
      "Epoch 35 Batch 50 Loss 0.9440\n",
      "Epoch 35 Batch 100 Loss 0.9198\n",
      "Epoch 35 Batch 150 Loss 0.9298\n",
      "Epoch 35 Batch 200 Loss 1.0160\n",
      "Epoch 35 Batch 250 Loss 0.9929\n",
      "Epoch 35 Batch 300 Loss 1.0583\n",
      "Epoch 35 Batch 350 Loss 0.9871\n",
      "Epoch 35 Batch 400 Loss 1.0990\n",
      "Epoch 35 Batch 450 Loss 1.1552\n",
      "Epoch 35 Batch 500 Loss 0.9656\n",
      "Epoch 35 Batch 550 Loss 1.0008\n",
      "Epoch 35 Final Loss 0.9941\n",
      "Time taken: 295.17 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 1.0458\n",
      "Epoch 36 Batch 50 Loss 0.9137\n",
      "Epoch 36 Batch 100 Loss 0.9746\n",
      "Epoch 36 Batch 150 Loss 0.9465\n",
      "Epoch 36 Batch 200 Loss 0.9359\n",
      "Epoch 36 Batch 250 Loss 1.0116\n",
      "Epoch 36 Batch 300 Loss 0.9314\n",
      "Epoch 36 Batch 350 Loss 1.0437\n",
      "Epoch 36 Batch 400 Loss 0.9618\n",
      "Epoch 36 Batch 450 Loss 1.0102\n",
      "Epoch 36 Batch 500 Loss 0.9377\n",
      "Epoch 36 Batch 550 Loss 1.0133\n",
      "Epoch 36 Final Loss 0.9790\n",
      "Time taken: 262.75 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.8783\n",
      "Epoch 37 Batch 50 Loss 0.9807\n",
      "Epoch 37 Batch 100 Loss 0.9069\n",
      "Epoch 37 Batch 150 Loss 1.0820\n",
      "Epoch 37 Batch 200 Loss 0.8729\n",
      "Epoch 37 Batch 250 Loss 0.8940\n",
      "Epoch 37 Batch 300 Loss 1.0492\n",
      "Epoch 37 Batch 350 Loss 0.9469\n",
      "Epoch 37 Batch 400 Loss 1.0848\n",
      "Epoch 37 Batch 450 Loss 0.9211\n",
      "Epoch 37 Batch 500 Loss 0.9567\n",
      "Epoch 37 Batch 550 Loss 0.9530\n",
      "Epoch 37 Final Loss 0.9643\n",
      "Time taken: 235.53 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.9298\n",
      "Epoch 38 Batch 50 Loss 1.0837\n",
      "Epoch 38 Batch 100 Loss 0.9970\n",
      "Epoch 38 Batch 150 Loss 0.8464\n",
      "Epoch 38 Batch 200 Loss 1.0090\n",
      "Epoch 38 Batch 250 Loss 0.9285\n",
      "Epoch 38 Batch 300 Loss 1.0558\n",
      "Epoch 38 Batch 350 Loss 0.9361\n",
      "Epoch 38 Batch 400 Loss 1.0355\n",
      "Epoch 38 Batch 450 Loss 0.9736\n",
      "Epoch 38 Batch 500 Loss 0.8790\n",
      "Epoch 38 Batch 550 Loss 1.2983\n",
      "Epoch 38 Final Loss 0.9505\n",
      "Time taken: 271.97 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.9339\n",
      "Epoch 39 Batch 50 Loss 0.8896\n",
      "Epoch 39 Batch 100 Loss 0.9833\n",
      "Epoch 39 Batch 150 Loss 0.8306\n",
      "Epoch 39 Batch 200 Loss 0.8879\n",
      "Epoch 39 Batch 250 Loss 0.9568\n",
      "Epoch 39 Batch 300 Loss 0.9373\n",
      "Epoch 39 Batch 350 Loss 0.8573\n",
      "Epoch 39 Batch 400 Loss 1.0342\n",
      "Epoch 39 Batch 450 Loss 0.9379\n",
      "Epoch 39 Batch 500 Loss 0.9684\n",
      "Epoch 39 Batch 550 Loss 0.9729\n",
      "Epoch 39 Final Loss 0.9389\n",
      "Time taken: 263.23 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.8098\n",
      "Epoch 40 Batch 50 Loss 0.8695\n",
      "Epoch 40 Batch 100 Loss 0.8562\n",
      "Epoch 40 Batch 150 Loss 1.0018\n",
      "Epoch 40 Batch 200 Loss 0.8827\n",
      "Epoch 40 Batch 250 Loss 0.7178\n",
      "Epoch 40 Batch 300 Loss 0.9523\n",
      "Epoch 40 Batch 350 Loss 0.8391\n",
      "Epoch 40 Batch 400 Loss 0.9459\n",
      "Epoch 40 Batch 450 Loss 0.8537\n",
      "Epoch 40 Batch 500 Loss 0.8202\n",
      "Epoch 40 Batch 550 Loss 0.8449\n",
      "Epoch 40 Final Loss 0.9253\n",
      "Time taken: 247.34 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.8364\n",
      "Epoch 41 Batch 50 Loss 0.8851\n",
      "Epoch 41 Batch 100 Loss 0.9357\n",
      "Epoch 41 Batch 150 Loss 0.9259\n",
      "Epoch 41 Batch 200 Loss 0.8556\n",
      "Epoch 41 Batch 250 Loss 0.8554\n",
      "Epoch 41 Batch 300 Loss 0.9150\n",
      "Epoch 41 Batch 350 Loss 0.9624\n",
      "Epoch 41 Batch 400 Loss 0.8929\n",
      "Epoch 41 Batch 450 Loss 0.9113\n",
      "Epoch 41 Batch 500 Loss 0.8607\n",
      "Epoch 41 Batch 550 Loss 0.9215\n",
      "Epoch 41 Final Loss 0.9106\n",
      "Time taken: 259.63 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.8136\n",
      "Epoch 42 Batch 50 Loss 0.8599\n",
      "Epoch 42 Batch 100 Loss 0.8156\n",
      "Epoch 42 Batch 150 Loss 0.8574\n",
      "Epoch 42 Batch 200 Loss 0.9123\n",
      "Epoch 42 Batch 250 Loss 1.2159\n",
      "Epoch 42 Batch 300 Loss 0.8822\n",
      "Epoch 42 Batch 350 Loss 0.8758\n",
      "Epoch 42 Batch 400 Loss 0.9870\n",
      "Epoch 42 Batch 450 Loss 0.8684\n",
      "Epoch 42 Batch 500 Loss 0.8203\n",
      "Epoch 42 Batch 550 Loss 0.8886\n",
      "Epoch 42 Final Loss 0.8972\n",
      "Time taken: 260.53 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.9480\n",
      "Epoch 43 Batch 50 Loss 0.8118\n",
      "Epoch 43 Batch 100 Loss 0.7992\n",
      "Epoch 43 Batch 150 Loss 0.8914\n",
      "Epoch 43 Batch 200 Loss 0.8817\n",
      "Epoch 43 Batch 250 Loss 0.8989\n",
      "Epoch 43 Batch 300 Loss 0.8726\n",
      "Epoch 43 Batch 350 Loss 1.1021\n",
      "Epoch 43 Batch 400 Loss 0.8746\n",
      "Epoch 43 Batch 450 Loss 0.9100\n",
      "Epoch 43 Batch 500 Loss 0.8755\n",
      "Epoch 43 Batch 550 Loss 0.9172\n",
      "Epoch 43 Final Loss 0.8867\n",
      "Time taken: 292.74 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.8694\n",
      "Epoch 44 Batch 50 Loss 0.8470\n",
      "Epoch 44 Batch 100 Loss 0.9336\n",
      "Epoch 44 Batch 150 Loss 0.8917\n",
      "Epoch 44 Batch 200 Loss 0.8382\n",
      "Epoch 44 Batch 250 Loss 1.0054\n",
      "Epoch 44 Batch 300 Loss 0.9268\n",
      "Epoch 44 Batch 350 Loss 1.0952\n",
      "Epoch 44 Batch 400 Loss 0.8894\n",
      "Epoch 44 Batch 450 Loss 0.7851\n",
      "Epoch 44 Batch 500 Loss 0.8278\n",
      "Epoch 44 Batch 550 Loss 0.9377\n",
      "Epoch 44 Final Loss 0.8731\n",
      "Time taken: 287.94 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.8602\n",
      "Epoch 45 Batch 50 Loss 0.8829\n",
      "Epoch 45 Batch 100 Loss 0.8641\n",
      "Epoch 45 Batch 150 Loss 0.8619\n",
      "Epoch 45 Batch 200 Loss 0.7305\n",
      "Epoch 45 Batch 250 Loss 0.8526\n",
      "Epoch 45 Batch 300 Loss 0.8976\n",
      "Epoch 45 Batch 350 Loss 0.8569\n",
      "Epoch 45 Batch 400 Loss 0.8947\n",
      "Epoch 45 Batch 450 Loss 0.9160\n",
      "Epoch 45 Batch 500 Loss 0.8010\n",
      "Epoch 45 Batch 550 Loss 0.9485\n",
      "Epoch 45 Final Loss 0.8623\n",
      "Time taken: 284.86 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.8043\n",
      "Epoch 46 Batch 50 Loss 0.7515\n",
      "Epoch 46 Batch 100 Loss 0.8171\n",
      "Epoch 46 Batch 150 Loss 0.8461\n",
      "Epoch 46 Batch 200 Loss 0.7125\n",
      "Epoch 46 Batch 250 Loss 0.8585\n",
      "Epoch 46 Batch 300 Loss 0.7727\n",
      "Epoch 46 Batch 350 Loss 0.8082\n",
      "Epoch 46 Batch 400 Loss 0.9563\n",
      "Epoch 46 Batch 450 Loss 0.8697\n",
      "Epoch 46 Batch 500 Loss 0.8536\n",
      "Epoch 46 Batch 550 Loss 0.9152\n",
      "Epoch 46 Final Loss 0.8516\n",
      "Time taken: 280.29 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.7508\n",
      "Epoch 47 Batch 50 Loss 0.8416\n",
      "Epoch 47 Batch 100 Loss 0.8125\n",
      "Epoch 47 Batch 150 Loss 0.9493\n",
      "Epoch 47 Batch 200 Loss 0.8123\n",
      "Epoch 47 Batch 250 Loss 0.8445\n",
      "Epoch 47 Batch 300 Loss 0.9197\n",
      "Epoch 47 Batch 350 Loss 0.8535\n",
      "Epoch 47 Batch 400 Loss 0.9081\n",
      "Epoch 47 Batch 450 Loss 1.0128\n",
      "Epoch 47 Batch 500 Loss 0.7960\n",
      "Epoch 47 Batch 550 Loss 0.8336\n",
      "Epoch 47 Final Loss 0.8379\n",
      "Time taken: 279.09 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.8544\n",
      "Epoch 48 Batch 50 Loss 0.8164\n",
      "Epoch 48 Batch 100 Loss 0.8707\n",
      "Epoch 48 Batch 150 Loss 0.8149\n",
      "Epoch 48 Batch 200 Loss 0.7049\n",
      "Epoch 48 Batch 250 Loss 0.7638\n",
      "Epoch 48 Batch 300 Loss 0.8438\n",
      "Epoch 48 Batch 350 Loss 0.8400\n",
      "Epoch 48 Batch 400 Loss 0.8614\n",
      "Epoch 48 Batch 450 Loss 0.8137\n",
      "Epoch 48 Batch 500 Loss 0.8717\n",
      "Epoch 48 Batch 550 Loss 0.7869\n",
      "Epoch 48 Final Loss 0.8266\n",
      "Time taken: 265.43 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.7365\n",
      "Epoch 49 Batch 50 Loss 0.7489\n",
      "Epoch 49 Batch 100 Loss 0.9216\n",
      "Epoch 49 Batch 150 Loss 0.8372\n",
      "Epoch 49 Batch 200 Loss 0.6912\n",
      "Epoch 49 Batch 250 Loss 0.8731\n",
      "Epoch 49 Batch 300 Loss 0.8136\n",
      "Epoch 49 Batch 350 Loss 0.8079\n",
      "Epoch 49 Batch 400 Loss 0.8517\n",
      "Epoch 49 Batch 450 Loss 0.7345\n",
      "Epoch 49 Batch 500 Loss 0.8079\n",
      "Epoch 49 Batch 550 Loss 0.7768\n",
      "Epoch 49 Final Loss 0.8155\n",
      "Time taken: 270.90 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.7689\n",
      "Epoch 50 Batch 50 Loss 0.8007\n",
      "Epoch 50 Batch 100 Loss 0.8056\n",
      "Epoch 50 Batch 150 Loss 0.8065\n",
      "Epoch 50 Batch 200 Loss 0.7901\n",
      "Epoch 50 Batch 250 Loss 0.7627\n",
      "Epoch 50 Batch 300 Loss 0.7760\n",
      "Epoch 50 Batch 350 Loss 0.8144\n",
      "Epoch 50 Batch 400 Loss 0.7777\n",
      "Epoch 50 Batch 450 Loss 0.7695\n",
      "Epoch 50 Batch 500 Loss 0.6997\n",
      "Epoch 50 Batch 550 Loss 0.8636\n",
      "Epoch 50 Final Loss 0.8052\n",
      "Time taken: 279.14 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING MODEL\n",
    "# ============================================================================\n",
    "EPOCHS =50\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss = train_step(img_tensor, target)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "\n",
    "    ckpt_manager.save()\n",
    "    print(f'Epoch {epoch+1} Final Loss {total_loss/len(dataset):.4f}')\n",
    "    print(f'Time taken: {time.time()-start:.2f} sec\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
