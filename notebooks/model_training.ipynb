{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUsya-quPcIb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ouTP859PaMz",
        "outputId": "228cea88-9639-4599-dc3f-f71534d0efd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PATH CONFIGURATION\n",
        "# ============================================================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CSV_PATH = '/content/drive/MyDrive/Kuliah/KecerdasanBuatan/FP/data/features/captions_preprocessed.csv'\n",
        "IMAGE_FEATURES_DIR = '/content/drive/MyDrive/Kuliah/KecerdasanBuatan/FP/data/images'\n",
        "TOKENIZER_PATH = '/content/drive/MyDrive/Kuliah/KecerdasanBuatan/FP/data/features/tokenizer.pkl'\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/Kuliah/KecerdasanBuatan/FP/checkpoints\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I9XvpxoPgmH"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOAD TOKENIZER & DATASET\n",
        "# ============================================================================\n",
        "# Memuat data tokenizer\n",
        "with open(TOKENIZER_PATH, 'rb') as f:\n",
        "    tokenizer_data = pickle.load(f)\n",
        "\n",
        "word_to_idx = tokenizer_data['word_to_idx']\n",
        "vocab_size = tokenizer_data['vocab_size']\n",
        "max_length = tokenizer_data['max_length']\n",
        "\n",
        "# Memuat dataset\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "EMBEDDING_DIM = 256\n",
        "UNITS = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn0UwhVnPtOg"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA GENERATOR\n",
        "# ============================================================================\n",
        "def load_data(img_name, caption):\n",
        "    # Menyesuaikan penamaan file .npy\n",
        "    path_npy = os.path.join(IMAGE_FEATURES_DIR, img_name.decode('utf-8') + '.npy')\n",
        "    img_tensor = np.load(path_npy)\n",
        "\n",
        "    # Tokenisasi caption\n",
        "    cap_seq = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in caption.decode('utf-8').split()]\n",
        "    cap_seq = tf.keras.preprocessing.sequence.pad_sequences([cap_seq], maxlen=max_length, padding='post')[0]\n",
        "\n",
        "    return img_tensor, cap_seq\n",
        "\n",
        "def map_func(img_name, cap):\n",
        "    img_tensor, cap_seq = tf.numpy_function(load_data, [img_name, cap], [tf.float32, tf.int32])\n",
        "    img_tensor.set_shape((64, 2048)) # Sesuai output InceptionV3 yang di-reshape\n",
        "    cap_seq.set_shape((max_length,))\n",
        "    return img_tensor, cap_seq\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_df['image'].values, train_df['caption'].values))\n",
        "dataset = dataset.map(map_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt0NICTIP10o"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE\n",
        "# ============================================================================\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "        # Menangani range fitur (0-17) agar stabil\n",
        "        self.ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        # Mencegah Dead ReLU pada fitur yang banyak nol\n",
        "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.ln(x)\n",
        "        return self.leaky_relu(x)\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True)\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        context_vector, _ = self.attention(features, hidden)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        return self.fc2(x), state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbebzZ0EQDvP",
        "outputId": "d41aac12-b93f-4dd9-ac91-5aae514a778e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TRAINING SETUP\n",
        "# ============================================================================\n",
        "encoder = CNN_Encoder(EMBEDDING_DIM)\n",
        "decoder = RNN_Decoder(EMBEDDING_DIM, UNITS, vocab_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # Masking padding index 0\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / (tf.reduce_sum(mask) + 1e-8)\n",
        "\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_PATH, max_to_keep=5)\n",
        "\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = tf.zeros((target.shape[0], UNITS))\n",
        "    dec_input = tf.expand_dims([word_to_idx['startseq']] * target.shape[0], 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "        for i in range(1, target.shape[1]):\n",
        "            predictions, hidden = decoder(dec_input, features, hidden)\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # Hitung gradien\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    # Gradient Clipping untuk mencegah NaN\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7fZeYfkQH8Z",
        "outputId": "6a3bb40c-22b2-45ce-8c2d-6c64246c2fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training...\n",
            "Epoch 1 Batch 0 Loss 3.7136\n",
            "Epoch 1 Batch 50 Loss 2.7689\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TRAINING MODEL\n",
        "# ============================================================================\n",
        "EPOCHS = 20\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss = train_step(img_tensor, target)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "    ckpt_manager.save()\n",
        "    print(f'Epoch {epoch+1} Final Loss {total_loss/len(dataset):.4f}')\n",
        "    print(f'Time taken: {time.time()-start:.2f} sec\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
